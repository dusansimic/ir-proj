{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from llama_cpp import Llama\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/Users/vladco/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaPrompter:\n",
    "    def __init__(self, base_template):\n",
    "        self.model = Llama(\n",
    "            model_path=MODEL_PATH,\n",
    "            n_threads=10,\n",
    "            n_batch=512,\n",
    "            n_gpu_layers=32\n",
    "        )\n",
    "        self.base_template = base_template\n",
    "\n",
    "    def prepare_prompt(self, **kwargs):\n",
    "        return self.base_template.format(**kwargs)\n",
    "\n",
    "    def prompt_model(self, prompt):\n",
    "        response=self.model(\n",
    "            prompt=prompt, \n",
    "            max_tokens=256,\n",
    "            temperature=0.5,\n",
    "            top_p=0.95,\n",
    "            repeat_penalty=1.2,\n",
    "            top_k=150,\n",
    "            echo=True)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaParser:\n",
    "    def __init__(self, parse_function):\n",
    "        self.parse_function = parse_function\n",
    "\n",
    "    def parse_output(self, model_output):\n",
    "        return self.parse_function(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaSession:\n",
    "    def __init__(self, paragraph):\n",
    "        self.base_template_1 = '''SYSTEM: You are a humble learning assistant trained to generate questions and provide answers based on the provided text. Be clear and concise.\n",
    "\n",
    "USER: Generate three question and corresponding answers based on the following paragraph.\n",
    "Output should be in format: \n",
    "Q: [your question]\n",
    "A: [your answer to your question]\n",
    "Context: {paragraph}\n",
    "\n",
    "ASSISTANT:\n",
    "'''\n",
    "        self.base_template_2 = '''SYSTEM: You are a humble learning assistant trained to analyze learner answer on provided text and corresponding question. Be clear and concise.\n",
    "\n",
    "USER: Based on the following paragraph, question, correct answer and user answer, tell to user if his answer was correct, and very briefly explain why.\n",
    "Paragraph: {paragraph}\n",
    "Question: {question}\n",
    "Correct answer: {correct_answer}\n",
    "User answer: {user_answer}\n",
    "\n",
    "ASSISTANT:\n",
    "'''\n",
    "        self.prompter = LlamaPrompter(self.base_template_1)\n",
    "        self.prompter_grader = LlamaPrompter(self.base_template_2)\n",
    "        self.parser_questions_answers = LlamaParser(self.parse_questions_answers)\n",
    "        self.parser_grades = LlamaParser(self.parse_grades)\n",
    "        self.questions = None\n",
    "        self.answers = None\n",
    "        self.paragraph = paragraph\n",
    "\n",
    "    def start_session(self):\n",
    "        prompt = self.prompter.prepare_prompt(paragraph=self.paragraph)\n",
    "        # print(prompt)\n",
    "        raw_output = self.prompter.prompt_model(prompt)\n",
    "        # print(raw_output)\n",
    "        self.questions, self.answers = self.parser_questions_answers.parse_output(raw_output['choices'][0]['text'])\n",
    "\n",
    "    def receive_answers(self, user_answers, idx):\n",
    "        prompt = self.prompter_grader.prepare_prompt(\n",
    "            paragraph=self.paragraph,\n",
    "            question=self.questions[idx],\n",
    "            correct_answer=self.answers[idx],\n",
    "            user_answer=user_answers\n",
    "        )\n",
    "        print('prompt:', prompt, '\\n')\n",
    "        raw_output = self.prompter_grader.prompt_model(prompt)\n",
    "        print(raw_output['choices'][0]['text'])\n",
    "        grading_result = self.parser_grades.parse_output(raw_output['choices'][0]['text'])\n",
    "        \n",
    "        return grading_result\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_questions_answers(model_output):\n",
    "        # parsira i prvo Q i A koji su placeholderi, zajdeb'o sam se, treba prvo split to asistentu\n",
    "        questions = re.findall(r\"Q: (.*?)\\s+A:\", model_output)\n",
    "        answers = re.findall(r\"A: (.*?)(?=Q:|$)\", model_output, flags=re.DOTALL)\n",
    "        return questions[1:], [answer.strip() for answer in answers][1:]\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_grades(model_output):\n",
    "        return model_output.split('ASSISTANT:')[1].strip()\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.questions, self.answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_clean.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/vladco/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      "llama.cpp: loading model from /Users/vladco/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 9311.07 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =   75.35 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "session = LlamaSession(dataset.loc[0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15711.50 ms\n",
      "llama_print_timings:      sample time =   121.09 ms /   188 runs   (    0.64 ms per token,  1552.55 tokens per second)\n",
      "llama_print_timings: prompt eval time = 15711.35 ms /   264 tokens (   59.51 ms per token,    16.80 tokens per second)\n",
      "llama_print_timings:        eval time = 31382.46 ms /   187 runs   (  167.82 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time = 47411.19 ms\n"
     ]
    }
   ],
   "source": [
    "session.start_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['What architectural style does the school have?',\n",
       "  'What are the names of the statues located in front and next to the Main Building?',\n",
       "  'What is special about the Grotto located behind the basilica?'],\n",
       " ['The school has a Catholic character with a gold dome on top of its Main Building featuring a golden statue of Virgin Mary.',\n",
       "  'In front of the Main Building, there is a copper statue of Christ with arms upraised bearing the legend \"Venite Ad Me Omnes\". Next to the Main Building stands the Basilica of the Sacred Heart.',\n",
       "  'The Grotto is a replica of the grotto at Lourdes, France where Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. It serves as a Marian place of prayer and reflection.'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.questions, session.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Kod je takav da prima jedno pitanje, pa ga ocenjue. Zbog toga bi bilo najbolje da napravis tako da odstampas prvo pitanje, cekas odgovor, posaljes grader-u taj odgovor, a da u medjuvremenu, ti user-u das sledece pitanje da na njega odgovara, dok se ovo ocenjuje (treba mu tridesetak sekundi da oceni jedan odgovor).\n",
    "- ## Najveci problem je generisanje pitanja i odgovora, posto to traje 45-55 sekundi. Tu prosto moramo cekati ili namesti da neki majmun igra dok se ceka. Ono sto je dobro jeste da ce biti generisano sva tri para (pitanje, odgovor), pa ce gornji bullet biti moguc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_answers = ['nemam', 'blage', 'veze'] # ovo isto treba ti da pokupis od usera, samo ne sva tri odjednom (v. sta sam napisao u MD celiji iznad)\n",
    "user_answer = user_answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: SYSTEM: You are a humble learning assistant trained to analyze learner answer on provided text and corresponding question. Be clear and concise.\n",
      "\n",
      "USER: Based on the following paragraph, question, correct answer and user answer, tell to user if his answer was correct, and very briefly explain why.\n",
      "Paragraph: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: What architectural style does the school have?\n",
      "Correct answer: The school has a Catholic character with a gold dome on top of its Main Building featuring a golden statue of Virgin Mary.\n",
      "User answer: nemam\n",
      "\n",
      "ASSISTANT:\n",
      " \n",
      "\n",
      "SYSTEM: You are a humble learning assistant trained to analyze learner answer on provided text and corresponding question. Be clear and concise.\n",
      "\n",
      "USER: Based on the following paragraph, question, correct answer and user answer, tell to user if his answer was correct, and very briefly explain why.\n",
      "Paragraph: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question: What architectural style does the school have?\n",
      "Correct answer: The school has a Catholic character with a gold dome on top of its Main Building featuring a golden statue of Virgin Mary.\n",
      "User answer: nemam\n",
      "\n",
      "ASSISTANT:\n",
      "Your answer was incorrect, but I understand that you might be new to this topic. Here's why the correct answer is \"Catholic\": The architectural style of the school is Catholic because it features a gold dome on top of its Main Building with a golden statue of Virgin Mary, and also has a Basilica of the Sacred Heart immediately next to it. Additionally, the Grotto behind the basilica is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. All these elements are characteristic of Catholic architecture and design.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 12120.19 ms\n",
      "llama_print_timings:      sample time =    87.69 ms /   136 runs   (    0.64 ms per token,  1550.88 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12120.11 ms /   304 tokens (   39.87 ms per token,    25.08 tokens per second)\n",
      "llama_print_timings:        eval time = 22449.40 ms /   135 runs   (  166.29 ms per token,     6.01 tokens per second)\n",
      "llama_print_timings:       total time = 34796.82 ms\n"
     ]
    }
   ],
   "source": [
    "grading_result = session.receive_answers(user_answer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grading Results:\n",
      "Your answer was incorrect, but I understand that you might be new to this topic. Here's why the correct answer is \"Catholic\": The architectural style of the school is Catholic because it features a gold dome on top of its Main Building with a golden statue of Virgin Mary, and also has a Basilica of the Sacred Heart immediately next to it. Additionally, the Grotto behind the basilica is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. All these elements are characteristic of Catholic architecture and design.\n"
     ]
    }
   ],
   "source": [
    "print(\"Grading Results:\")\n",
    "print(grading_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
